{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No normalization for SPS. Feature removed!\n",
      "No normalization for AvgIpc. Feature removed!\n",
      "Skipped loading some Tensorflow models, missing a dependency. No module named 'tensorflow'\n",
      "Skipped loading modules with pytorch-geometric dependency, missing a dependency. No module named 'dgl'\n",
      "Skipped loading modules with transformers dependency. No module named 'transformers'\n",
      "cannot import name 'HuggingFaceModel' from 'deepchem.models.torch_models' (/opt/conda/envs/jmp/lib/python3.11/site-packages/deepchem/models/torch_models/__init__.py)\n",
      "Skipped loading some Jax models, missing a dependency. No module named 'jax'\n",
      "Skipped loading some PyTorch models, missing a dependency. No module named 'tensorflow'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id='mpo5imfg' trainer=TrainerConfig(optimizer=OptimizerConfig(log_grad_norm=True, gradient_clipping=GradientClippingConfig(value=1.0, algorithm='value')), supports_skip_batch_exception=False, supports_parameter_hooks=False, set_float32_matmul_precision='medium', precision='16-mixed', max_epochs=100000, max_time='07:00:00:00', inference_mode=False) meta={'ckpt_path': PosixPath('/mnt/shared/checkpoints/fm_gnoc_large_2_epoch.ckpt'), 'ema_backbone': True} train_dataset=FinetuneLmdbDatasetConfig(src=PosixPath('/mnt/shared/datasets/rmd17/lmdb/aspirin/train'), metadata_path=PosixPath('/mnt/shared/datasets/rmd17/lmdb/aspirin/train/metadata.npz')) val_dataset=FinetuneLmdbDatasetConfig(src=PosixPath('/mnt/shared/datasets/rmd17/lmdb/aspirin/val'), metadata_path=PosixPath('/mnt/shared/datasets/rmd17/lmdb/aspirin/val/metadata.npz')) test_dataset=FinetuneLmdbDatasetConfig(src=PosixPath('/mnt/shared/datasets/rmd17/lmdb/aspirin/test'), metadata_path=PosixPath('/mnt/shared/datasets/rmd17/lmdb/aspirin/test/metadata.npz')) optimizer=AdamWConfig(lr=5e-06, weight_decay=0.1, betas=(0.9, 0.95)) lr_scheduler=WarmupCosRLPConfig(warmup_epochs=5, max_epochs=32, warmup_start_lr_factor=0.1, min_lr_factor=0.1, rlp=RLPConfig(patience=25, factor=0.8)) backbone=BackboneConfig(num_spherical=7, num_radial=128, num_blocks=6, emb_size_atom=256, emb_size_edge=1024, emb_size_trip_in=64, emb_size_trip_out=128, emb_size_quad_in=64, emb_size_quad_out=32, emb_size_aint_in=64, emb_size_aint_out=64, emb_size_rbf=32, emb_size_cbf=16, emb_size_sbf=64, num_before_skip=2, num_after_skip=2, num_concat=4, num_atom=3, num_output_afteratom=3, num_atom_emb_layers=2, regress_forces=False, sbf={'name': 'legendre_outer'}, quad_interaction=True, atom_edge_interaction=True, edge_atom_interaction=True, atom_interaction=True, qint_tags=[1, 2], absolute_rbf_cutoff=12.0, dropout=None, edge_dropout=None) batch_size=4 primary_metric=PrimaryMetricConfig(name='force_mae', mode='min') early_stopping=EarlyStoppingConfig(patience=1000, min_lr=1e-10) normalization={'y': NormalizationConfig(mean=-17617.379355234374, std=0.2673998440577667), 'force': NormalizationConfig(mean=0.0, std=1.2733363)} parameter_specific_optimizers=[ParamSpecificOptimizerConfig(paremeter_patterns=['embedding.*'], optimizer=AdamWConfig(lr=1.5e-06, weight_decay=0.1, betas=(0.9, 0.95)), lr_scheduler=WarmupCosRLPConfig(warmup_epochs=5, max_epochs=32, warmup_start_lr_factor=0.1, min_lr_factor=0.33333333333333337, rlp=RLPConfig(patience=3, factor=0.8))), ParamSpecificOptimizerConfig(paremeter_patterns=['backbone.int_blocks.0.*', 'backbone.out_blocks.1.*', 'backbone.out_blocks.0.*'], optimizer=AdamWConfig(lr=2.7500000000000004e-06, weight_decay=0.1, betas=(0.9, 0.95)), lr_scheduler=WarmupCosRLPConfig(warmup_epochs=5, max_epochs=32, warmup_start_lr_factor=0.1, min_lr_factor=0.18181818181818182, rlp=RLPConfig(patience=3, factor=0.8))), ParamSpecificOptimizerConfig(paremeter_patterns=['backbone.int_blocks.1.*', 'backbone.out_blocks.2.*'], optimizer=AdamWConfig(lr=2.0000000000000003e-06, weight_decay=0.1, betas=(0.9, 0.95)), lr_scheduler=WarmupCosRLPConfig(warmup_epochs=5, max_epochs=32, warmup_start_lr_factor=0.1, min_lr_factor=0.25, rlp=RLPConfig(patience=3, factor=0.8))), ParamSpecificOptimizerConfig(paremeter_patterns=['backbone.int_blocks.2.*', 'backbone.out_blocks.3.*'], optimizer=AdamWConfig(lr=1.5e-06, weight_decay=0.1, betas=(0.9, 0.95)), lr_scheduler=WarmupCosRLPConfig(warmup_epochs=5, max_epochs=32, warmup_start_lr_factor=0.1, min_lr_factor=0.33333333333333337, rlp=RLPConfig(patience=3, factor=0.8))), ParamSpecificOptimizerConfig(paremeter_patterns=['backbone.int_blocks.3.*', 'backbone.out_blocks.4.*'], optimizer=AdamWConfig(lr=2.0000000000000003e-06, weight_decay=0.1, betas=(0.9, 0.95)), lr_scheduler=WarmupCosRLPConfig(warmup_epochs=5, max_epochs=32, warmup_start_lr_factor=0.1, min_lr_factor=0.25, rlp=RLPConfig(patience=3, factor=0.8))), ParamSpecificOptimizerConfig(paremeter_patterns=['backbone.int_blocks.4.*', 'backbone.out_blocks.5.*'], optimizer=AdamWConfig(lr=2.7500000000000004e-06, weight_decay=0.1, betas=(0.9, 0.95)), lr_scheduler=WarmupCosRLPConfig(warmup_epochs=5, max_epochs=32, warmup_start_lr_factor=0.1, min_lr_factor=0.18181818181818182, rlp=RLPConfig(patience=3, factor=0.8))), ParamSpecificOptimizerConfig(paremeter_patterns=['backbone.int_blocks.5.*', 'backbone.out_blocks.6.*'], optimizer=AdamWConfig(lr=3.125e-06, weight_decay=0.1, betas=(0.9, 0.95)), lr_scheduler=WarmupCosRLPConfig(warmup_epochs=5, max_epochs=32, warmup_start_lr_factor=0.1, min_lr_factor=0.16, rlp=RLPConfig(patience=3, factor=0.8)))] gradient_forces=True model_type='forces' molecule='aspirin'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/repositories/fm/src/jmp/lightning/model/config.py:709: BaseConfig._rng is None. The generated IDs will not be reproducible. To fix this, call BaseConfig.set_seed(...) before generating any IDs.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Copyright (c) Meta Platforms, Inc. and affiliates.\n",
    "All rights reserved.\n",
    "\n",
    "This source code is licensed under the license found in the\n",
    "LICENSE file in the root directory of this source tree.\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from jmp.configs.finetune.jmp_l import jmp_l_ft_config_\n",
    "from jmp.configs.finetune.rmd17 import jmp_l_rmd17_config_\n",
    "from jmp.tasks.finetune.base import FinetuneConfigBase, FinetuneModelBase\n",
    "from jmp.tasks.finetune.rmd17 import RMD17Config, RMD17Model\n",
    "\n",
    "ckpt_path = Path(\"/mnt/shared/checkpoints/fm_gnoc_large_2_epoch.ckpt\")\n",
    "base_path = Path(\"/mnt/shared/datasets/rmd17/\")\n",
    "\n",
    "# We create a list of all configurations that we want to run.\n",
    "configs: list[tuple[FinetuneConfigBase, type[FinetuneModelBase]]] = []\n",
    "\n",
    "config = RMD17Config.draft()\n",
    "jmp_l_ft_config_(config, ckpt_path)  # This loads the base JMP-L fine-tuning config\n",
    "# This loads the rMD17-specific configuration\n",
    "jmp_l_rmd17_config_(config, \"aspirin\", base_path)\n",
    "config = config.finalize()  # Actually construct the config object\n",
    "print(config)\n",
    "\n",
    "configs.append((config, RMD17Model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6a4164e0219458c8b37bde34b2ae030",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fast dev run:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to import rich. Falling back to default Python logging.\n",
      "CRITICAL:jmp.lightning.trainer.trainer:Setting config.trainer.default_root_dir='/workspaces/repositories/fm/config/lightning_logs/3aghcddl'.\n",
      "Seed set to 0\n",
      "CRITICAL:jmp.lightning.util.seed:Set global seed to 0.\n",
      "CRITICAL:jmp.lightning.runner:Auto-wrapping run in Trainer context\n",
      "CRITICAL:jmp.tasks.finetune.base:Using regular backbone\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unrecognized arguments:  dict_keys(['learnable_rbf', 'learnable_rbf_stds', 'unique_basis_per_layer', 'dropout', 'edge_dropout'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CRITICAL:jmp.tasks.finetune.base:Freezing 0 parameters (0.00%) out of 160,874,752 total parameters (160,874,752 trainable)\n",
      "CRITICAL:jmp.utils.finetune_state_dict:Loaded 585 EMA parameters\n",
      "CRITICAL:jmp.utils.finetune_state_dict:Loaded state dict from /mnt/shared/checkpoints/fm_gnoc_large_2_epoch.ckpt\n",
      "CRITICAL:jmp.utils.state_dict:pattern='out_blocks.0.scale_rbf_F.*' matched keys ['out_blocks.0.scale_rbf_F.scale_factor'], which were ignored during loading.\n",
      "CRITICAL:jmp.utils.state_dict:pattern='out_blocks.0.seq_forces.*' matched keys ['out_blocks.0.seq_forces.0.dense_mlp.0.linear.weight', 'out_blocks.0.seq_forces.0.dense_mlp.1.linear.weight', 'out_blocks.0.seq_forces.1.dense_mlp.0.linear.weight', 'out_blocks.0.seq_forces.1.dense_mlp.1.linear.weight', 'out_blocks.0.seq_forces.2.dense_mlp.0.linear.weight', 'out_blocks.0.seq_forces.2.dense_mlp.1.linear.weight'], which were ignored during loading.\n",
      "CRITICAL:jmp.utils.state_dict:pattern='out_blocks.0.dense_rbf_F.*' matched keys ['out_blocks.0.dense_rbf_F.linear.weight'], which were ignored during loading.\n",
      "CRITICAL:jmp.utils.state_dict:pattern='out_blocks.1.scale_rbf_F.*' matched keys ['out_blocks.1.scale_rbf_F.scale_factor'], which were ignored during loading.\n",
      "CRITICAL:jmp.utils.state_dict:pattern='out_blocks.1.seq_forces.*' matched keys ['out_blocks.1.seq_forces.0.dense_mlp.0.linear.weight', 'out_blocks.1.seq_forces.0.dense_mlp.1.linear.weight', 'out_blocks.1.seq_forces.1.dense_mlp.0.linear.weight', 'out_blocks.1.seq_forces.1.dense_mlp.1.linear.weight', 'out_blocks.1.seq_forces.2.dense_mlp.0.linear.weight', 'out_blocks.1.seq_forces.2.dense_mlp.1.linear.weight'], which were ignored during loading.\n",
      "CRITICAL:jmp.utils.state_dict:pattern='out_blocks.1.dense_rbf_F.*' matched keys ['out_blocks.1.dense_rbf_F.linear.weight'], which were ignored during loading.\n",
      "CRITICAL:jmp.utils.state_dict:pattern='out_blocks.2.scale_rbf_F.*' matched keys ['out_blocks.2.scale_rbf_F.scale_factor'], which were ignored during loading.\n",
      "CRITICAL:jmp.utils.state_dict:pattern='out_blocks.2.seq_forces.*' matched keys ['out_blocks.2.seq_forces.0.dense_mlp.0.linear.weight', 'out_blocks.2.seq_forces.0.dense_mlp.1.linear.weight', 'out_blocks.2.seq_forces.1.dense_mlp.0.linear.weight', 'out_blocks.2.seq_forces.1.dense_mlp.1.linear.weight', 'out_blocks.2.seq_forces.2.dense_mlp.0.linear.weight', 'out_blocks.2.seq_forces.2.dense_mlp.1.linear.weight'], which were ignored during loading.\n",
      "CRITICAL:jmp.utils.state_dict:pattern='out_blocks.2.dense_rbf_F.*' matched keys ['out_blocks.2.dense_rbf_F.linear.weight'], which were ignored during loading.\n",
      "CRITICAL:jmp.utils.state_dict:pattern='out_blocks.3.scale_rbf_F.*' matched keys ['out_blocks.3.scale_rbf_F.scale_factor'], which were ignored during loading.\n",
      "CRITICAL:jmp.utils.state_dict:pattern='out_blocks.3.seq_forces.*' matched keys ['out_blocks.3.seq_forces.0.dense_mlp.0.linear.weight', 'out_blocks.3.seq_forces.0.dense_mlp.1.linear.weight', 'out_blocks.3.seq_forces.1.dense_mlp.0.linear.weight', 'out_blocks.3.seq_forces.1.dense_mlp.1.linear.weight', 'out_blocks.3.seq_forces.2.dense_mlp.0.linear.weight', 'out_blocks.3.seq_forces.2.dense_mlp.1.linear.weight'], which were ignored during loading.\n",
      "CRITICAL:jmp.utils.state_dict:pattern='out_blocks.3.dense_rbf_F.*' matched keys ['out_blocks.3.dense_rbf_F.linear.weight'], which were ignored during loading.\n",
      "CRITICAL:jmp.utils.state_dict:pattern='out_blocks.4.scale_rbf_F.*' matched keys ['out_blocks.4.scale_rbf_F.scale_factor'], which were ignored during loading.\n",
      "CRITICAL:jmp.utils.state_dict:pattern='out_blocks.4.seq_forces.*' matched keys ['out_blocks.4.seq_forces.0.dense_mlp.0.linear.weight', 'out_blocks.4.seq_forces.0.dense_mlp.1.linear.weight', 'out_blocks.4.seq_forces.1.dense_mlp.0.linear.weight', 'out_blocks.4.seq_forces.1.dense_mlp.1.linear.weight', 'out_blocks.4.seq_forces.2.dense_mlp.0.linear.weight', 'out_blocks.4.seq_forces.2.dense_mlp.1.linear.weight'], which were ignored during loading.\n",
      "CRITICAL:jmp.utils.state_dict:pattern='out_blocks.4.dense_rbf_F.*' matched keys ['out_blocks.4.dense_rbf_F.linear.weight'], which were ignored during loading.\n",
      "CRITICAL:jmp.utils.state_dict:pattern='out_blocks.5.scale_rbf_F.*' matched keys ['out_blocks.5.scale_rbf_F.scale_factor'], which were ignored during loading.\n",
      "CRITICAL:jmp.utils.state_dict:pattern='out_blocks.5.seq_forces.*' matched keys ['out_blocks.5.seq_forces.0.dense_mlp.0.linear.weight', 'out_blocks.5.seq_forces.0.dense_mlp.1.linear.weight', 'out_blocks.5.seq_forces.1.dense_mlp.0.linear.weight', 'out_blocks.5.seq_forces.1.dense_mlp.1.linear.weight', 'out_blocks.5.seq_forces.2.dense_mlp.0.linear.weight', 'out_blocks.5.seq_forces.2.dense_mlp.1.linear.weight'], which were ignored during loading.\n",
      "CRITICAL:jmp.utils.state_dict:pattern='out_blocks.5.dense_rbf_F.*' matched keys ['out_blocks.5.dense_rbf_F.linear.weight'], which were ignored during loading.\n",
      "CRITICAL:jmp.utils.state_dict:pattern='out_blocks.6.scale_rbf_F.*' matched keys ['out_blocks.6.scale_rbf_F.scale_factor'], which were ignored during loading.\n",
      "CRITICAL:jmp.utils.state_dict:pattern='out_blocks.6.seq_forces.*' matched keys ['out_blocks.6.seq_forces.0.dense_mlp.0.linear.weight', 'out_blocks.6.seq_forces.0.dense_mlp.1.linear.weight', 'out_blocks.6.seq_forces.1.dense_mlp.0.linear.weight', 'out_blocks.6.seq_forces.1.dense_mlp.1.linear.weight', 'out_blocks.6.seq_forces.2.dense_mlp.0.linear.weight', 'out_blocks.6.seq_forces.2.dense_mlp.1.linear.weight'], which were ignored during loading.\n",
      "CRITICAL:jmp.utils.state_dict:pattern='out_blocks.6.dense_rbf_F.*' matched keys ['out_blocks.6.dense_rbf_F.linear.weight'], which were ignored during loading.\n",
      "CRITICAL:jmp.utils.state_dict:pattern='out_mlp_F.*' matched keys ['out_mlp_F.out_mlp.0.linear.weight', 'out_mlp_F.out_mlp.1.dense_mlp.0.linear.weight', 'out_mlp_F.out_mlp.1.dense_mlp.1.linear.weight', 'out_mlp_F.out_mlp.2.dense_mlp.0.linear.weight', 'out_mlp_F.out_mlp.2.dense_mlp.1.linear.weight'], which were ignored during loading.\n",
      "CRITICAL:jmp.tasks.finetune.base:Loaded backbone state dict (backbone and embedding).\n",
      "CRITICAL:jmp.lightning.trainer.trainer:Disabling loggers because fast_dev_run is enabled.\n",
      "CRITICAL:jmp.lightning.trainer.trainer:Setting num_nodes to 1 (no SLURM detected).\n",
      "CRITICAL:jmp.lightning.trainer.trainer:LightningTrainer.__init__ with args=() and kwargs={'accelerator': 'auto', 'strategy': 'auto', 'devices': 'auto', 'num_nodes': 1, 'precision': '16-mixed', 'logger': None, 'fast_dev_run': 1, 'max_epochs': 100000, 'min_epochs': None, 'max_steps': -1, 'min_steps': None, 'max_time': '07:00:00:00', 'limit_train_batches': None, 'limit_val_batches': None, 'limit_test_batches': None, 'limit_predict_batches': None, 'overfit_batches': 0.0, 'val_check_interval': None, 'check_val_every_n_epoch': 1, 'num_sanity_val_steps': None, 'log_every_n_steps': 50, 'enable_checkpointing': None, 'enable_progress_bar': None, 'enable_model_summary': None, 'accumulate_grad_batches': 1, 'deterministic': None, 'benchmark': None, 'inference_mode': False, 'use_distributed_sampler': True, 'detect_anomaly': False, 'barebones': False, 'plugins': [], 'sync_batchnorm': False, 'reload_dataloaders_every_n_epochs': 0, 'gradient_clip_algorithm': 'value', 'gradient_clip_val': 1.0, 'default_root_dir': '/workspaces/repositories/fm/config/lightning_logs/3aghcddl', 'callbacks': [<lightning.pytorch.callbacks.on_exception_checkpoint.OnExceptionCheckpoint object at 0x7c4b4a2235d0>]}.\n",
      "Trainer will use only 1 of 2 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=2)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/conda/envs/jmp/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:75: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "Running in `fast_dev_run` mode: will run the requested loop using 1 batch(es). Logging and checkpointing is suppressed.\n",
      "WARNING:jmp.lightning.trainer.logging:Logger DummyLogger does not support run_id, ignoring.\n",
      "CRITICAL:jmp.lightning.trainer.trainer:LightningTrainer log directory: None.\n",
      "/opt/conda/envs/jmp/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/checkpoint_connector.py:126: `.fit(ckpt_path=None)` was called without a model. The last model of the previous `fit` call will be used. You can pass `fit(ckpt_path='best')` to use the best model or `fit(ckpt_path='last')` to use the last model. If you pass a value, this warning will be silenced.\n",
      "/opt/conda/envs/jmp/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/checkpoint_connector.py:186: .fit(ckpt_path=\"last\") is set, but there is no last checkpoint available. No checkpoint will be loaded. HINT: Set `ModelCheckpoint(..., save_last=True)`.\n",
      "WARNING:jmp.lightning.model.modules.wandb:Could not find wandb logger or module to log\n",
      "CRITICAL:jmp.lightning.model.base:Fast dev run detected, setting debug flag to True.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "CRITICAL:jmp.tasks.config:Optimizer: AdamW\n",
      "Optimizer kwargs: {}\n",
      "Base kwargs: {'lr': 5e-06, 'amsgrad': False, 'weight_decay': 0.1, 'betas': (0.9, 0.95), 'eps': 1e-08}\n",
      "Param groups: Param group 0:\n",
      "    Params: 1\n",
      "    Total param size: 30720\n",
      "    Other kwargs: {'lr': 1.5e-06, 'amsgrad': False, 'weight_decay': 0.1, 'betas': (0.9, 0.95), 'eps': 1e-08, 'name': 'embedding.*', 'foreach': None, 'maximize': False, 'capturable': False, 'differentiable': False, 'fused': None}\n",
      "Param group 1:\n",
      "    Params: 85\n",
      "    Total param size: 27354112\n",
      "    Other kwargs: {'lr': 2.7500000000000004e-06, 'amsgrad': False, 'weight_decay': 0.1, 'betas': (0.9, 0.95), 'eps': 1e-08, 'name': 'backbone.int_blocks.0.*,backbone.out_blocks.1.*,backbone.out_blocks.0.*', 'foreach': None, 'maximize': False, 'capturable': False, 'differentiable': False, 'fused': None}\n",
      "Param group 2:\n",
      "    Params: 71\n",
      "    Total param size: 26272768\n",
      "    Other kwargs: {'lr': 2.0000000000000003e-06, 'amsgrad': False, 'weight_decay': 0.1, 'betas': (0.9, 0.95), 'eps': 1e-08, 'name': 'backbone.int_blocks.1.*,backbone.out_blocks.2.*', 'foreach': None, 'maximize': False, 'capturable': False, 'differentiable': False, 'fused': None}\n",
      "Param group 3:\n",
      "    Params: 71\n",
      "    Total param size: 26272768\n",
      "    Other kwargs: {'lr': 1.5e-06, 'amsgrad': False, 'weight_decay': 0.1, 'betas': (0.9, 0.95), 'eps': 1e-08, 'name': 'backbone.int_blocks.2.*,backbone.out_blocks.3.*', 'foreach': None, 'maximize': False, 'capturable': False, 'differentiable': False, 'fused': None}\n",
      "Param group 4:\n",
      "    Params: 71\n",
      "    Total param size: 26272768\n",
      "    Other kwargs: {'lr': 2.0000000000000003e-06, 'amsgrad': False, 'weight_decay': 0.1, 'betas': (0.9, 0.95), 'eps': 1e-08, 'name': 'backbone.int_blocks.3.*,backbone.out_blocks.4.*', 'foreach': None, 'maximize': False, 'capturable': False, 'differentiable': False, 'fused': None}\n",
      "Param group 5:\n",
      "    Params: 71\n",
      "    Total param size: 26272768\n",
      "    Other kwargs: {'lr': 2.7500000000000004e-06, 'amsgrad': False, 'weight_decay': 0.1, 'betas': (0.9, 0.95), 'eps': 1e-08, 'name': 'backbone.int_blocks.4.*,backbone.out_blocks.5.*', 'foreach': None, 'maximize': False, 'capturable': False, 'differentiable': False, 'fused': None}\n",
      "Param group 6:\n",
      "    Params: 71\n",
      "    Total param size: 26272768\n",
      "    Other kwargs: {'lr': 3.125e-06, 'amsgrad': False, 'weight_decay': 0.1, 'betas': (0.9, 0.95), 'eps': 1e-08, 'name': 'backbone.int_blocks.5.*,backbone.out_blocks.6.*', 'foreach': None, 'maximize': False, 'capturable': False, 'differentiable': False, 'fused': None}\n",
      "Param group 7:\n",
      "    Params: 23\n",
      "    Total param size: 2126080\n",
      "    Other kwargs: {'lr': 5e-06, 'amsgrad': False, 'weight_decay': 0.1, 'betas': (0.9, 0.95), 'eps': 1e-08, 'name': 'rest', 'foreach': None, 'maximize': False, 'capturable': False, 'differentiable': False, 'fused': None}\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "CRITICAL:jmp.tasks.finetune.base:Computed warmup_steps: 5\n",
      "CRITICAL:jmp.tasks.finetune.base:Computed max_steps: 32\n",
      "CRITICAL:jmp.tasks.finetune.base:Computed warmup_steps: 5\n",
      "CRITICAL:jmp.tasks.finetune.base:Computed max_steps: 32\n",
      "CRITICAL:jmp.tasks.finetune.base:Computed warmup_steps: 5\n",
      "CRITICAL:jmp.tasks.finetune.base:Computed max_steps: 32\n",
      "CRITICAL:jmp.tasks.finetune.base:Computed warmup_steps: 5\n",
      "CRITICAL:jmp.tasks.finetune.base:Computed max_steps: 32\n",
      "CRITICAL:jmp.tasks.finetune.base:Computed warmup_steps: 5\n",
      "CRITICAL:jmp.tasks.finetune.base:Computed max_steps: 32\n",
      "CRITICAL:jmp.tasks.finetune.base:Computed warmup_steps: 5\n",
      "CRITICAL:jmp.tasks.finetune.base:Computed max_steps: 32\n",
      "CRITICAL:jmp.tasks.finetune.base:Computed warmup_steps: 5\n",
      "CRITICAL:jmp.tasks.finetune.base:Computed max_steps: 32\n",
      "CRITICAL:jmp.tasks.finetune.base:Computed warmup_steps: 5\n",
      "CRITICAL:jmp.tasks.finetune.base:Computed max_steps: 32\n",
      "CRITICAL:jmp.tasks.finetune.base:param_group_lr_scheduler_settings=[{'warmup_epochs': 5, 'max_epochs': 32, 'warmup_start_lr': 1.5000000000000002e-07, 'eta_min': 5.000000000000001e-07, 'should_restart': False}, {'warmup_epochs': 5, 'max_epochs': 32, 'warmup_start_lr': 2.7500000000000007e-07, 'eta_min': 5.000000000000001e-07, 'should_restart': False}, {'warmup_epochs': 5, 'max_epochs': 32, 'warmup_start_lr': 2.0000000000000004e-07, 'eta_min': 5.000000000000001e-07, 'should_restart': False}, {'warmup_epochs': 5, 'max_epochs': 32, 'warmup_start_lr': 1.5000000000000002e-07, 'eta_min': 5.000000000000001e-07, 'should_restart': False}, {'warmup_epochs': 5, 'max_epochs': 32, 'warmup_start_lr': 2.0000000000000004e-07, 'eta_min': 5.000000000000001e-07, 'should_restart': False}, {'warmup_epochs': 5, 'max_epochs': 32, 'warmup_start_lr': 2.7500000000000007e-07, 'eta_min': 5.000000000000001e-07, 'should_restart': False}, {'warmup_epochs': 5, 'max_epochs': 32, 'warmup_start_lr': 3.125e-07, 'eta_min': 5.000000000000001e-07, 'should_restart': False}, {'warmup_epochs': 5, 'max_epochs': 32, 'warmup_start_lr': 5.000000000000001e-07, 'eta_min': 5.000000000000001e-07, 'should_restart': False}]\n",
      "/opt/conda/envs/jmp/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | embedding     | Embedding        | 30.7 K\n",
      "1 | backbone      | GemNetOCBackbone | 160 M \n",
      "2 | out_energy    | Sequential       | 262 K \n",
      "3 | train_metrics | FinetuneMetrics  | 0     \n",
      "4 | val_metrics   | FinetuneMetrics  | 0     \n",
      "5 | test_metrics  | FinetuneMetrics  | 0     \n",
      "---------------------------------------------------\n",
      "160 M     Trainable params\n",
      "0         Non-trainable params\n",
      "160 M     Total params\n",
      "643.499   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0677ceeb641846debdd9d821699a5611",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/jmp/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbf45028fd7043c4bbd8f91ea2f0cdde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=1` reached.\n",
      "CRITICAL:jmp.lightning.trainer.trainer:Ran 1 finalizers for Trainer cleanup.\n",
      "Seed set to 0\n",
      "CRITICAL:jmp.lightning.util.seed:Reset global seed.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from jmp.lightning import Runner, Trainer\n",
    "from jmp.utils.finetune_state_dict import (\n",
    "    filter_state_dict,\n",
    "    retreive_state_dict_for_finetuning,\n",
    ")\n",
    "\n",
    "\n",
    "def run(config: FinetuneConfigBase, model_cls: type[FinetuneModelBase]) -> None:\n",
    "    if (ckpt_path := config.meta.get(\"ckpt_path\")) is None:\n",
    "        raise ValueError(\"No checkpoint path provided\")\n",
    "\n",
    "    model = model_cls(config)\n",
    "\n",
    "    # Load the checkpoint\n",
    "    state_dict = retreive_state_dict_for_finetuning(\n",
    "        ckpt_path, load_emas=config.meta.get(\"ema_backbone\", False)\n",
    "    )\n",
    "    embedding = filter_state_dict(state_dict, \"embedding.atom_embedding.\")\n",
    "    backbone = filter_state_dict(state_dict, \"backbone.\")\n",
    "    model.load_backbone_state_dict(backbone=backbone, embedding=embedding, strict=True)\n",
    "\n",
    "    trainer = Trainer(config)\n",
    "    trainer.fit(model)\n",
    "\n",
    "\n",
    "runner = Runner(run)\n",
    "runner.fast_dev_run(configs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
