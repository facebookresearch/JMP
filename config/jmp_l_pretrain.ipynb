{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "id='fy0th22x' trainer=TrainerConfig(optimizer=OptimizerConfig(log_grad_norm=True, gradient_clipping=GradientClippingConfig(value=1.0)), supports_skip_batch_exception=False, supports_parameter_hooks=False, set_float32_matmul_precision='medium', precision='16-mixed', use_distributed_sampler=False) optimizer=AdamWConfig(lr=0.0003, weight_decay=0.1, betas=(0.9, 0.95)) lr_scheduler=LinearWarmupCosineAnnealingSchedulerConfig(warmup_steps=2000, max_epochs=2, warmup_start_lr_factor=0.2, min_lr_factor=0.1) edge_dropout=0.1 backbone=BackboneConfig(num_spherical=7, num_radial=128, num_blocks=4, emb_size_atom=256, emb_size_edge=512, emb_size_trip_in=64, emb_size_trip_out=64, emb_size_quad_in=32, emb_size_quad_out=32, emb_size_aint_in=64, emb_size_aint_out=64, emb_size_rbf=16, emb_size_cbf=16, emb_size_sbf=32, num_before_skip=2, num_after_skip=2, num_concat=1, num_atom=3, num_output_afteratom=3, num_atom_emb_layers=2, direct_forces=True, sbf={'name': 'legendre_outer'}, quad_interaction=True, atom_edge_interaction=True, edge_atom_interaction=True, atom_interaction=True, qint_tags=[1, 2], absolute_rbf_cutoff=12.0, dropout=None, edge_dropout=0.1) batch_size=4 num_workers=8 tasks=[TaskConfig(name='oc20', train_dataset=PretrainDatasetConfig(src=PosixPath('/datasets/s2ef/2M/train'), metadata_path=PosixPath('/datasets/s2ef/2M/train_metadata.npz')), val_dataset=PretrainDatasetConfig(src=PosixPath('/datasets/s2ef/all/val_id'), metadata_path=PosixPath('/datasets/s2ef/all/val_id_metadata.npz')), force_loss_scale=73.0, normalization={'y': NormalizationConfig(mean=0.0, std=24.901469505465872), 'force': NormalizationConfig(mean=0.0, std=0.5111534595489502)}), TaskConfig(name='oc22', train_dataset=PretrainDatasetConfig(src=PosixPath('/shared/pre-training-datasets/oc22/s2ef-total/train'), metadata_path=PosixPath('/shared/pre-training-datasets/oc22/s2ef-total/train/metadata.npz')), val_dataset=PretrainDatasetConfig(src=PosixPath('/shared/pre-training-datasets/oc22/s2ef-total/val_id'), metadata_path=PosixPath('/shared/pre-training-datasets/oc22/s2ef-total/val_id/metadata.npz')), force_loss_scale=80.0, normalization={'y': NormalizationConfig(mean=0.0, std=25.229595396538468), 'force': NormalizationConfig(mean=0.0, std=0.25678861141204834)}), TaskConfig(name='ani1x', train_dataset=PretrainDatasetConfig(src=PosixPath('/shared/pre-training-datasets/ani1x/train'), metadata_path=PosixPath('/shared/pre-training-datasets/ani1x/train/metadata.npz')), val_dataset=PretrainDatasetConfig(src=PosixPath('/shared/pre-training-datasets/ani1x/val'), metadata_path=PosixPath('/shared/pre-training-datasets/ani1x/val/metadata.npz')), force_loss_scale=15.0, normalization={'y': NormalizationConfig(mean=0.0, std=2.8700712783472118), 'force': NormalizationConfig(mean=0.0, std=2.131422996520996)}), TaskConfig(name='transition1x', train_dataset=PretrainDatasetConfig(src=PosixPath('/shared/pre-training-datasets/trans1x/train'), metadata_path=PosixPath('/shared/pre-training-datasets/trans1x/train/metadata.npz')), val_dataset=PretrainDatasetConfig(src=PosixPath('/shared/pre-training-datasets/trans1x/val'), metadata_path=PosixPath('/shared/pre-training-datasets/trans1x/val/metadata.npz')), force_loss_scale=14.0, normalization={'y': NormalizationConfig(mean=0.0, std=1.787466168382901), 'force': NormalizationConfig(mean=0.0, std=0.3591422140598297)})] mt_dataset=MTDatasetConfig(sample_type='temperature', sample_temperature=2.0) ema=EMAConfig(decay=0.99)\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/workspaces/repositories/fm/src/ll/model/config.py:709: IdSeedWarning: BaseConfig._rng is None. The generated IDs will not be reproducible. To fix this, call BaseConfig.set_seed(...) before generating any IDs.\n",
                        "  warnings.warn(\n"
                    ]
                }
            ],
            "source": [
                "\"\"\"\n",
                "Copyright (c) Meta Platforms, Inc. and affiliates.\n",
                "All rights reserved.\n",
                "\n",
                "This source code is licensed under the license found in the\n",
                "LICENSE file in the root directory of this source tree.\n",
                "\"\"\"\n",
                "\n",
                "from pathlib import Path\n",
                "\n",
                "from jmp.configs.pretrain.jmp_l import jmp_l_pt_config_\n",
                "from jmp.tasks.pretrain import PretrainConfig, PretrainModel\n",
                "from jmp.tasks.pretrain.module import (\n",
                "    NormalizationConfig,\n",
                "    PretrainDatasetConfig,\n",
                "    TaskConfig,\n",
                ")\n",
                "\n",
                "\n",
                "# Let's make the config\n",
                "def jmp_l_config():\n",
                "    config = PretrainConfig.draft()\n",
                "\n",
                "    jmp_l_pt_config_(config)\n",
                "\n",
                "    # Set data config\n",
                "    config.batch_size = 4\n",
                "    config.num_workers = 8\n",
                "\n",
                "    # Set the tasks\n",
                "    config.tasks = [\n",
                "        TaskConfig(\n",
                "            name=\"oc20\",\n",
                "            train_dataset=PretrainDatasetConfig(\n",
                "                src=Path(\"/datasets/s2ef/2M/train/\"),\n",
                "                metadata_path=Path(\"/datasets/s2ef/2M/train_metadata.npz\"),\n",
                "            ),\n",
                "            val_dataset=PretrainDatasetConfig(\n",
                "                src=Path(\"/datasets/s2ef/all/val_id/\"),\n",
                "                metadata_path=Path(\"/datasets/s2ef/all/val_id_metadata.npz\"),\n",
                "            ),\n",
                "            energy_loss_scale=1.0,\n",
                "            force_loss_scale=73.0,\n",
                "            normalization={\n",
                "                \"y\": NormalizationConfig(mean=0.0, std=24.901469505465872),\n",
                "                \"force\": NormalizationConfig(mean=0.0, std=0.5111534595489502),\n",
                "            },\n",
                "        ),\n",
                "        TaskConfig(\n",
                "            name=\"oc22\",\n",
                "            train_dataset=PretrainDatasetConfig(\n",
                "                src=Path(\"/shared/pre-training-datasets/oc22/s2ef-total/train/\"),\n",
                "            ),\n",
                "            val_dataset=PretrainDatasetConfig(\n",
                "                src=Path(\"/shared/pre-training-datasets/oc22/s2ef-total/val_id/\"),\n",
                "            ),\n",
                "            energy_loss_scale=1.0,\n",
                "            force_loss_scale=80.0,\n",
                "            normalization={\n",
                "                \"y\": NormalizationConfig(mean=0.0, std=25.229595396538468),\n",
                "                \"force\": NormalizationConfig(mean=0.0, std=0.25678861141204834),\n",
                "            },\n",
                "        ),\n",
                "        TaskConfig(\n",
                "            name=\"ani1x\",\n",
                "            train_dataset=PretrainDatasetConfig(\n",
                "                src=Path(\"/shared/pre-training-datasets/ani1x/train/\"),\n",
                "            ),\n",
                "            val_dataset=PretrainDatasetConfig(\n",
                "                src=Path(\"/shared/pre-training-datasets/ani1x/val/\"),\n",
                "            ),\n",
                "            energy_loss_scale=1.0,\n",
                "            force_loss_scale=15.0,\n",
                "            normalization={\n",
                "                \"y\": NormalizationConfig(mean=0.0, std=2.8700712783472118),\n",
                "                \"force\": NormalizationConfig(mean=0.0, std=2.131422996520996),\n",
                "            },\n",
                "        ),\n",
                "        TaskConfig(\n",
                "            name=\"transition1x\",\n",
                "            train_dataset=PretrainDatasetConfig(\n",
                "                src=Path(\"/shared/pre-training-datasets/trans1x/train/\"),\n",
                "            ),\n",
                "            val_dataset=PretrainDatasetConfig(\n",
                "                src=Path(\"/shared/pre-training-datasets/trans1x/val/\"),\n",
                "            ),\n",
                "            energy_loss_scale=1.0,\n",
                "            force_loss_scale=14.0,\n",
                "            normalization={\n",
                "                \"y\": NormalizationConfig(mean=0.0, std=1.787466168382901),\n",
                "                \"force\": NormalizationConfig(mean=0.0, std=0.3591422140598297),\n",
                "            },\n",
                "        ),\n",
                "    ]\n",
                "\n",
                "    return config.finalize()\n",
                "\n",
                "\n",
                "config = jmp_l_config()\n",
                "print(config)\n",
                "\n",
                "configs: list[tuple[PretrainConfig, type[PretrainModel]]] = []\n",
                "configs.append((config, PretrainModel))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "abae91062ef54c5db60746cff798ddab",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Fast dev run:   0%|          | 0/1 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Failed to import rich. Falling back to default Python logging.\n",
                        "CRITICAL:ll.trainer.trainer:Setting config.trainer.default_root_dir='/workspaces/repositories/fm/config/lightning_logs/ns2igr3i'.\n",
                        "Seed set to 0\n",
                        "CRITICAL:ll.util.seed:Set global seed to 0.\n",
                        "CRITICAL:ll.runner:Auto-wrapping run in Trainer context\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Unrecognized arguments:  dict_keys(['learnable_rbf', 'learnable_rbf_stds', 'unique_basis_per_layer', 'dropout', 'edge_dropout'])\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "CRITICAL:ll.trainer.trainer:Disabling loggers because fast_dev_run is enabled.\n",
                        "CRITICAL:ll.trainer.trainer:Setting num_nodes to 1 (no SLURM detected).\n",
                        "CRITICAL:ll.trainer.trainer:LightningTrainer.__init__ with args=() and kwargs={'accelerator': 'auto', 'strategy': 'auto', 'devices': 'auto', 'num_nodes': 1, 'precision': '16-mixed', 'logger': None, 'fast_dev_run': 16, 'max_epochs': None, 'min_epochs': None, 'max_steps': -1, 'min_steps': None, 'max_time': None, 'limit_train_batches': None, 'limit_val_batches': None, 'limit_test_batches': None, 'limit_predict_batches': None, 'overfit_batches': 0.0, 'val_check_interval': None, 'check_val_every_n_epoch': 1, 'num_sanity_val_steps': None, 'log_every_n_steps': 50, 'enable_checkpointing': None, 'enable_progress_bar': None, 'enable_model_summary': None, 'accumulate_grad_batches': 1, 'deterministic': None, 'benchmark': None, 'inference_mode': True, 'use_distributed_sampler': False, 'detect_anomaly': False, 'barebones': False, 'plugins': [], 'sync_batchnorm': False, 'reload_dataloaders_every_n_epochs': 0, 'gradient_clip_algorithm': 'norm', 'gradient_clip_val': 1.0, 'default_root_dir': '/workspaces/repositories/fm/config/lightning_logs/ns2igr3i', 'callbacks': [<lightning.pytorch.callbacks.on_exception_checkpoint.OnExceptionCheckpoint object at 0x72f04ae15b10>]}.\n",
                        "Trainer will use only 1 of 2 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=2)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
                        "Using 16bit Automatic Mixed Precision (AMP)\n",
                        "GPU available: True (cuda), used: True\n",
                        "TPU available: False, using: 0 TPU cores\n",
                        "IPU available: False, using: 0 IPUs\n",
                        "HPU available: False, using: 0 HPUs\n",
                        "/opt/conda/envs/jmp/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:75: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
                        "Running in `fast_dev_run` mode: will run the requested loop using 16 batch(es). Logging and checkpointing is suppressed.\n",
                        "WARNING:ll.trainer.logging:Logger DummyLogger does not support run_id, ignoring.\n",
                        "CRITICAL:ll.trainer.trainer:LightningTrainer log directory: None.\n",
                        "/opt/conda/envs/jmp/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/checkpoint_connector.py:126: `.fit(ckpt_path=None)` was called without a model. The last model of the previous `fit` call will be used. You can pass `fit(ckpt_path='best')` to use the best model or `fit(ckpt_path='last')` to use the last model. If you pass a value, this warning will be silenced.\n",
                        "/opt/conda/envs/jmp/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/checkpoint_connector.py:186: .fit(ckpt_path=\"last\") is set, but there is no last checkpoint available. No checkpoint will be loaded. HINT: Set `ModelCheckpoint(..., save_last=True)`.\n",
                        "WARNING:ll.model.modules.wandb:Could not find wandb logger or module to log\n",
                        "CRITICAL:ll.model.base:Fast dev run detected, setting debug flag to True.\n",
                        "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
                        "CRITICAL:jmp.tasks.config:Optimizer: AdamW\n",
                        "Optimizer kwargs: {}\n",
                        "Base kwargs: {}\n",
                        "Param groups: Param group 0:\n",
                        "    Params: 405\n",
                        "    Total param size: 44131328\n",
                        "    Other kwargs: {'lr': 0.0003, 'amsgrad': False, 'weight_decay': 0.1, 'betas': (0.9, 0.95), 'eps': 1e-08, 'foreach': None, 'maximize': False, 'capturable': False, 'differentiable': False, 'fused': None}\n",
                        "Loading `train_dataloader` to estimate number of stepping batches.\n",
                        "CRITICAL:jmp.tasks.pretrain.module:Setting max_steps=32 by default.\n",
                        "\n",
                        "  | Name          | Type             | Params\n",
                        "---------------------------------------------------\n",
                        "0 | embedding     | Embedding        | 30.7 K\n",
                        "1 | backbone      | GemNetOCBackbone | 38.8 M\n",
                        "2 | output        | Output           | 5.3 M \n",
                        "3 | train_metrics | FMMetrics        | 0     \n",
                        "4 | val_metrics   | FMMetrics        | 0     \n",
                        "5 | task_steps    | TypedModuleDict  | 0     \n",
                        "---------------------------------------------------\n",
                        "44.1 M    Trainable params\n",
                        "0         Non-trainable params\n",
                        "44.1 M    Total params\n",
                        "176.525   Total estimated model params size (MB)\n",
                        "CRITICAL:jmp.modules.dataset.concat_dataset:Ignoring balancing because `ignore_balancing` is True in `MTSampledDataset.__init__`.\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "bdbcf5bdecbe4d5f8cb39dc296e4c116",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Training: |          | 0/? [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/opt/conda/envs/jmp/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
                        "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "0afc12edadcd42b9837f23922d632015",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Validation: |          | 0/? [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "`Trainer.fit` stopped: `max_steps=16` reached.\n",
                        "CRITICAL:ll.trainer.trainer:Ran 1 finalizers for Trainer cleanup.\n",
                        "Seed set to 0\n",
                        "CRITICAL:ll.util.seed:Reset global seed.\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "[None]"
                        ]
                    },
                    "execution_count": 3,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "from jmp.lightning import Runner, Trainer\n",
                "\n",
                "\n",
                "def run(config: PretrainConfig, model_cls: type[PretrainModel]) -> None:\n",
                "    model = model_cls(config)\n",
                "    trainer = Trainer(config)\n",
                "    trainer.fit(model)\n",
                "\n",
                "\n",
                "runner = Runner(run)\n",
                "runner.fast_dev_run(configs, n_batches=16)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "fm",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.6"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
